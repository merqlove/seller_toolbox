{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, Markdown as md\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from scrapinghub import ScrapinghubClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# загружаем конфиг\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные выгружаются напрямую из Scrapinghub по API. Нужный датасет (или даже нужные датасеты, их можно быть много) ищутся по их тегу. Чтобы все работало, в конфиге тулбокса (файл `config.yaml`) нужно в параметре `category_positions:job_tag_filter` указать название тэга выгрузки Scrapinghub, по которой нужно выгружать данные. Скрипт выгрузит результаты работы всех завершенных задач с указанным тегом. Тег в Scrapinghub нужно указать при создании выгрузки данных (job).\n",
    "\n",
    "Помимо этого в конфиге (файл `config.yaml`) в поле `seller:sku_list` должны быть перечислены Wildberries артикулы всех товаров, позиции которых нужно отслеживать.\n",
    "\n",
    "Дальше этот датасет будет сохранен в папку data директории, в которой находится блокнт. Если в папке уже лежит файл с названием вида `category_positions_*.csv`, то новые данные загружены не будут.\n",
    "\n",
    "Нужный файл можно положить и вручную, скачав его из Scrapinghub. Можно даже положить несколько файлов – скрипт возьмет последний по алфавиту.\n",
    "\n",
    "После изменения переменной с тегом в Юпитере нужно выбрать пункт меню \"Run -> Run All Cells\". Дальше блокнот сделает все сам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# здесь нужно указать тэг нужной выгрузки, по-умолчанию он берется из конфига\n",
    "job_tag_filter = config['category_positions']['job_tag_filter']\n",
    "\n",
    "# остальное лучше не трогать\n",
    "api_key = config['scrapinghub']['api_key']\n",
    "project_id = config['scrapinghub']['project_id']\n",
    "csv_name =  './data/category_positions_' + str(datetime.now()) + '.csv'\n",
    "item_ids = sorted(config['seller']['sku_list'])\n",
    "\n",
    "def load_data(job_tag_filter, api_key, project_id, csv_name):\n",
    "    client = ScrapinghubClient(api_key)\n",
    "    project = client.get_project(project_id)\n",
    "    fieldnames = []\n",
    "    \n",
    "    jobs_summary = project.jobs.iter(has_tag=[job_tag_filter], state='finished')\n",
    "    \n",
    "    # определяем поля выгрузки\n",
    "    for job in jobs_summary:\n",
    "        for item in client.get_job(job['key']).items.iter():\n",
    "            fieldnames = item.keys()\n",
    "            break\n",
    "        break\n",
    "    \n",
    "    # готовим CSV\n",
    "    with open(csv_name, 'w') as csvfile:\n",
    "        fieldnames = ['parse_date'] + item_ids\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        # собираем выгрущки по тегу evossa_10_min\n",
    "        jobs_summary = project.jobs.iter(has_tag=[job_tag_filter], state='finished')\n",
    "\n",
    "        for job in jobs_summary:\n",
    "            # все даты в API отдаются в виде таймстемпов в миллисекундах\n",
    "            job_time = datetime.utcfromtimestamp(job['running_time']/1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            print(\"Фильтруем выгрузки по тэгу {}\".format(job_tag_filter))\n",
    "            \n",
    "            print(\"Пишем данные выгрузки от {}\".format(str(job_time)))\n",
    "            \n",
    "            positions = {}\n",
    "\n",
    "            for item in client.get_job(job['key']).items.iter():\n",
    "                item_id = int(item['wb_id'])\n",
    "\n",
    "                if item_id in item_ids:\n",
    "                    positions[item_id] = item['wb_category_position']\n",
    "\n",
    "            positions['parse_date'] = job_time\n",
    "\n",
    "            writer.writerow(positions)\n",
    "\n",
    "load_data(job_tag_filter, api_key, project_id, csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "latest_csv = sorted(glob.glob('./data/category_positions_*.csv'), reverse=True)[0]\n",
    "data = pd.read_csv(latest_csv, index_col=['parse_date'], parse_dates=['parse_date'], dayfirst=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ позиций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Позиции всех запрошенных SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "data.plot(kind='line', ax=ax)\n",
    "        \n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0), loc='lower right', borderaxespad=0.)\n",
    "plt.ylim(0, 1900)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Позиции с нейтральной и изменяющейся динамикой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "std_threshold = 100\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].std() < std_threshold:\n",
    "        data.plot(kind='line', y=str(column), ax=ax)\n",
    "        \n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0), loc='lower right', borderaxespad=0.)\n",
    "plt.ylim(0, 1900)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].std() > std_threshold:\n",
    "        data.plot(kind='line', y=str(column), ax=ax)\n",
    "        \n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0), loc='lower right', borderaxespad=0.)\n",
    "plt.ylim(0, 1900)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Позиции с выбросами вверх и выбросами вниз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "podskok = data[data.std(axis=1) > std_threshold].copy()\n",
    "\n",
    "# транспонируем для удобства работы, потом вернем все обратно\n",
    "podskok_T = podskok.transpose()\n",
    "\n",
    "# вычисляем максимальный выброс и его сторону\n",
    "podskok_T['median'] = podskok_T.median(axis=1)\n",
    "podskok_T['max_upper'] = podskok_T.max(axis=1) - podskok_T.median(axis=1)\n",
    "podskok_T['max_lower'] = podskok_T.median(axis=1) - podskok_T.min(axis=1)\n",
    "podskok_T['max_diff'] = podskok_T[['max_upper', 'max_lower']].max(axis=1)\n",
    "podskok_T['max_diff_idx'] = podskok_T[['max_upper', 'max_lower']].idxmax(axis=1)\n",
    "\n",
    "# делим датасет на две части\n",
    "podskok_T_upper = podskok_T[podskok_T['max_diff_idx']=='max_upper']\n",
    "podskok_T_lower = podskok_T[podskok_T['max_diff_idx']=='max_lower']\n",
    "\n",
    "# удаляем вспомогательные столбцы, чтобы транспонировать обратно\n",
    "del(podskok_T_upper['max_upper'])\n",
    "del(podskok_T_upper['max_lower'])\n",
    "del(podskok_T_upper['max_diff'])\n",
    "del(podskok_T_upper['max_diff_idx'])\n",
    "\n",
    "del(podskok_T_lower['max_upper'])\n",
    "del(podskok_T_lower['max_lower'])\n",
    "del(podskok_T_lower['max_diff'])\n",
    "del(podskok_T_lower['max_diff_idx'])\n",
    "\n",
    "# транспонируем обратно и возвращаем даты\n",
    "podskok_upper = podskok_T_upper.transpose()\n",
    "podskok_lower = podskok_T_lower.transpose()\n",
    "\n",
    "# строим графики\n",
    "ax = plt.gca()\n",
    "\n",
    "podskok_upper.plot(kind='line', ax=ax)\n",
    "        \n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0), loc='lower right', borderaxespad=0.)\n",
    "plt.ylim(0, 1900)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for column in podskok_lower.columns:\n",
    "    if column != '7969923':\n",
    "        podskok_lower.plot(kind='line', y=str(column), ax=ax)\n",
    "        \n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0), loc='lower right', borderaxespad=0.)\n",
    "plt.ylim(0, 1900)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
